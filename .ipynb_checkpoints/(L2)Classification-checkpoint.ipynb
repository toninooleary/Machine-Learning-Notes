{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Instances and instance spaces\n",
    "* data consists of a set of instances\n",
    "    * An instance represents an object of interest.\n",
    "Instance refers to an individual item. to a person with cancer. Each person in the instance space is an 'instance'.\n",
    "\n",
    "## Instance space \n",
    "It's a set of all the instances!\n",
    "\n",
    "* e.g. set of all email messages written in english\n",
    "* set of human faces\n",
    "\n",
    "    ## Notation\n",
    "    instance space will be denoted by $X$\n",
    "    \n",
    "    An instance will be denoted as $x$\n",
    "    \n",
    "    $x \\in X$\n",
    "\n",
    "\n",
    "## Labels and Label Spaces\n",
    "In supervised problems, each instance is associated with a label.\n",
    "\n",
    "* Set of all labels is called the label space\n",
    "    * Label space $C$ in classification tasks., e.g. in binary classification you may have two labels in the label space --> $C = ${$frogs, badgers$}\n",
    "    * real numbers $\\subseteq R$ in regression tasks e.g. pixel coordinates\n",
    "    \n",
    "    ## Notation\n",
    "    We will denote label space by $Y$\n",
    "    \n",
    "    Labelling function $f$ : $X$ --> $Y$ maps from instances to labels\n",
    "    \n",
    "    Label associated with a given instance $x$ denoted by $f(x)$\n",
    "\n",
    "\n",
    "## Binary Classification:\n",
    "\n",
    "* Classify as one thing or another. An image classified as a male or female.\n",
    "* \"It labels instances as one class label or another\"\n",
    "\n",
    "## Learning a classifier\n",
    "\n",
    "* we have a set of training data each of them are labelled.\n",
    "* You get an input and $f$ will turn it into an output.\n",
    "* We do not know the true classification function $f$ (A function that will classify inputs to the correct output 100% of the time)\n",
    "* The process of machine learning is try to aproximate this function $f$ which we call $\\hat{f}$.\n",
    "* We are trying to learn a model $\\hat{f}$ that maps instances to class labels, having seen a few examples that it has been trained on.\n",
    "* It can never be trained on every image possible. Therefore, you get a function $\\hat{f}$ that tries to understand the whole instance space $X$ given a few examples.\n",
    "\n",
    "## For a classified binary classifier: Accuracy:\n",
    "\n",
    "* $\\hat{f}$ will aproximate the true classification function $f$\n",
    "    * $\\hat{f}$ can never be exactly the same function as $f$\n",
    "    * You can never say your model is 100% correct and $\\hat{f}$ will make errors assigning class labels to instances.\n",
    "\n",
    "## Determining accuracy\n",
    "Accuracy depends on what you test your model on!\n",
    "You use a certain amount of your labeled examples to train on. Then you test on new examples it has never seen.\n",
    "\n",
    "## Validation\n",
    "We care about whether our classifier has learned to **generalise**. (generalisation means can classify examples it has never seen before)\n",
    "\n",
    "* Typically you will save 20% of your training data for testing the classifiers accuracy. This is called a **validation set**. This data will not be use in training\n",
    "* The data chose for validation must be randomly choosen data from your training set. \n",
    "    * You wouldn't put all your difficult data in your training set and all your easy data in your validation(testing) set. (This is referred to as \"independently and identically distributed (or 'idd'). \")\n",
    "    * If the validation set is unusual it will give as a poor measure of how accurate our classfier is.\n",
    "    \n",
    "    ### Overfitting\n",
    "    * When your data is over trained on training data. It may get 99% on the original data set but only 50% on unseen datasets. This means our model doesn't generalise well from our training data to unseen data.\n",
    "    [Good explaination on overfitting!](https://elitedatascience.com/overfitting-in-machine-learning \"Covers overfitting vs. underfitting, how to detect it and how to prevent it!\")\n",
    "    \n",
    "    ### trade-offs when creating a validation set\n",
    "    * If you have 100 labelled instances, and you take half to validate with, then you have half the data to train on. The classifier will do less well. To figure out a balance work out how varied yor training data samples are. You want the training and testing data to both capture this variablility. Maybe you might have dogs, cats, humans etc. CAPTURE THE VARIABILITY\n",
    "    \n",
    "## K-Fold Cross-Validation\n",
    "\n",
    "You split your data into sections, Each colour in the diagram represents a section.\n",
    "\n",
    "We train the data three times, each time we use two sections for training and a different section for validation.\n",
    "\n",
    "It removes some variability, on deciding which is used for training and which for validation.\n",
    "\n",
    "![Diagram](./pictures/k_fold.png \"K-fold diagram\")\n",
    "\n",
    "## peeking and maintaining a test set\n",
    "\n",
    "* You might use validation to decide what model to use. Therefore, you might want to keep a test set seperate from everything to evaluate final performance.\n",
    "\n",
    "    ### Peeking\n",
    "    If we mix training/validation/testing data. Then it will over-inflate how well we think our model will perform. This is called peeking.\n",
    "\n",
    "## Data insights(insights are inputs):\n",
    "* If your inputs are rubbish, your outputs will be rubbish\n",
    "* Do curation of it, go into your data and inspect\n",
    "* Inspect your data manually, look at how your data is distributed using data reduction tools.\n",
    "* Data labels always contain mistakes or ambiguities. Don't always trust them\n",
    "\n",
    "## Confusion Matrix (contingency table)\n",
    "\n",
    "good for binary classification \n",
    "\n",
    "![Diagram](./pictures/confusion_matrix.PNG \"Confusion Matrix\")\n",
    "\n",
    "diagonal = number of times classified correct\n",
    "off diagonal = number of times classified wrong\n",
    "\n",
    "working out accuracy:\n",
    "\n",
    "![Diagram](./pictures/accuracy_and_error.PNG \"Calculating accuracy and error rate\")\n",
    "    ### True positive:\n",
    "    * proportion of correctly predicted positive cases\n",
    "    \n",
    "    ### True negative:\n",
    "    * proportion of correctly predicted negative cases\n",
    "\n",
    "![Diagram](./pictures/tpr_and_tnr.png \"Calculating true positive and true negative rates\")\n",
    "    \n",
    "    ### False positive rate:\n",
    "    * When it misidentifies a case\n",
    "    \n",
    "    ### False negative rate:\n",
    "    * incorectly predicted positive case\n",
    "    \n",
    "![Diagram](./pictures/fpr_and_fnr.png \"Calculating false positive and false negative rates\") \n",
    "\n",
    "    ### Precision:\n",
    "    * proportion of positively predictions that are correct\n",
    "    \n",
    "    ### Recall:\n",
    "    * proportion of positive instaces that are predicted\n",
    "    \n",
    "![Diagram](./pictures/fpr_and_fnr.png \"Calculating precision and recall\")     \n",
    "    \n",
    "## proritising accuracy\n",
    "* Each task will need different measurements from the confusion matrix. It may be detrmental to misidentify a prediction. e.g. for cancer patient.\n",
    "\n",
    "OVO and FINISH OTHERS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
